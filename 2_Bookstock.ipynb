{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn\n",
    "import scipy\n",
    "import torch\n",
    "import torchvision\n",
    "import os\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take sales of only one product for now\n",
    "pd_bookstock= pd.read_csv('dataset_2019_01_24.csv')\n",
    "\n",
    "df_p = pd_bookstock[pd_bookstock['base_product_number']==54046134]\n",
    "#print(df_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['retail_outlet_number', 'base_product_number', 'calendar_date',\n",
      "       'calendar_hour', 'stock_count_shelf', 'stock_count_backroom',\n",
      "       'book_stock_quantity', 'change', 'change_cause', 'min_bookstock',\n",
      "       'max_bookstock', 'calculated_bookstock', 'change_cause_grouped'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(df_p.columns) #copy-pasting column names from here to groupby() functions avoids KeyErrors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days of sales data: 142\n",
      "Average number of sales per day: -4.887323943661972\n",
      "    calendar_date  sales\n",
      "0      01/06/2018      9\n",
      "1      01/07/2018      2\n",
      "2      01/09/2018      5\n",
      "3      01/10/2018      5\n",
      "4      02/06/2018      9\n",
      "5      02/09/2018      2\n",
      "6      02/10/2018     10\n",
      "7      03/06/2018      8\n",
      "8      03/07/2018      4\n",
      "9      03/08/2018      2\n",
      "10     03/09/2018      5\n",
      "11     03/10/2018      4\n",
      "12     04/06/2018      3\n",
      "13     04/07/2018      4\n",
      "14     04/08/2018      6\n",
      "15     04/09/2018      6\n",
      "16     04/10/2018     11\n",
      "17     05/06/2018      7\n",
      "18     05/07/2018      4\n",
      "19     05/08/2018      2\n",
      "20     05/10/2018      9\n",
      "21     06/06/2018     10\n",
      "22     06/07/2018     10\n",
      "23     06/08/2018      2\n",
      "24     06/09/2018      1\n",
      "25     06/10/2018      5\n",
      "26     07/06/2018      1\n",
      "27     07/07/2018      3\n",
      "28     07/08/2018      9\n",
      "29     07/09/2018      6\n",
      "..            ...    ...\n",
      "112    24/10/2018      2\n",
      "113    25/06/2018      1\n",
      "114    25/07/2018      2\n",
      "115    25/08/2018      6\n",
      "116    25/09/2018      6\n",
      "117    25/10/2018      6\n",
      "118    26/08/2018      3\n",
      "119    26/09/2018      6\n",
      "120    26/10/2018      5\n",
      "121    27/06/2018      4\n",
      "122    27/07/2018      5\n",
      "123    27/08/2018      2\n",
      "124    27/09/2018      6\n",
      "125    27/10/2018      3\n",
      "126    28/06/2018      4\n",
      "127    28/08/2018      4\n",
      "128    28/09/2018      9\n",
      "129    28/10/2018      4\n",
      "130    29/06/2018      2\n",
      "131    29/07/2018      2\n",
      "132    29/09/2018      4\n",
      "133    29/10/2018      6\n",
      "134    30/06/2018      5\n",
      "135    30/07/2018      2\n",
      "136    30/08/2018      6\n",
      "137    30/09/2018      5\n",
      "138    30/10/2018      3\n",
      "139    31/07/2018      2\n",
      "140    31/08/2018      5\n",
      "141    31/10/2018      3\n",
      "\n",
      "[142 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#Take only entries that have change_cause as 'sales'\n",
    "sales_df_p = df_p[df_p['change_cause']=='sales'].reset_index()\n",
    "#print(sales_df_p)\n",
    "\n",
    "#sum up all sales within a particular day for all days. This gives per day sales for product 54046134 for all days\n",
    "# Note that the total data for this product is for 142 days\n",
    "sum_change = sales_df_p.groupby('calendar_date')['change'].sum().reset_index()\n",
    "max_calcbk = sales_df_p.groupby('calendar_date')['calculated_bookstock'].min().reset_index()\n",
    "#print(sum_change)\n",
    "#print(max_calcbk)\n",
    "#sum_change['max_calcbk'] = max_calcbk['calculated_bookstock'] #add a new column named 'max_calcbk'\n",
    "sum_change['change'] = sum_change['change']*-1\n",
    "\n",
    "##################################################\n",
    "#Additional info:\n",
    "size_days = np.size(sum_change['calendar_date'])\n",
    "mean_sales = sum_change['change'].mean()*-1\n",
    "print(\"Total number of days of sales data:\", size_days)\n",
    "print(\"Average number of sales per day:\",mean_sales) #calculating average sales per day\n",
    "###################################################\n",
    "\n",
    "#rename the column from 'change' to 'sales'\n",
    "sum_change.rename(columns={list(sum_change)[1]:\"sales\"}, inplace = True) # can successfully rename column here\n",
    "\n",
    "#sum_change.head(71)\n",
    "size_days_half = np.size(sum_change['calendar_date'])/2\n",
    "\n",
    "#print(sum_change.head(71))\n",
    "#print(sum_change.tail(71))\n",
    "print(sum_change)\n",
    "\n",
    "training_dt = sum_change.head(71) #create the training dataset of upper half entries\n",
    "testing_dt = sum_change.tail(71) #testing dataset of bottom half entries\n",
    "\n",
    "#CREATE LABELS for all sales numbers\n",
    "all_labels=[]\n",
    "for a in range(np.size(sum_change['sales'])):\n",
    "    if(sum_change['sales'][a]<5):\n",
    "        all_labels.append('0')\n",
    "    else:\n",
    "        all_labels.append('1')\n",
    "\n",
    "#print(all_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days of sales data: 142\n",
      "9\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of days of sales data:\",size_days)\n",
    "#sum_change[:75] #accessing the first 75 entries of table sum_change\n",
    "#print(sum_change['change']) #accessing the column\n",
    "print(sum_change['sales'][0]) #accessing a column value by its index\n",
    "\n",
    "#Create training set\n",
    "training_set_less = []\n",
    "training_set_more = []\n",
    "labels = []\n",
    "\n",
    "for a in range(np.size(training_dt['calendar_date'])):\n",
    "    if(training_dt['sales'][a]<5):\n",
    "#        print(training_dt['sales'][a], training_dt['max_calcbk'][a])\n",
    "        training_set_less.append((training_dt['sales'][a], training_dt['max_calcbk'][a]))\n",
    "        labels.append('0')\n",
    "\n",
    "for a in range(np.size(training_dt['calendar_date'])):\n",
    "    if(training_dt['sales'][a]>=5):\n",
    "#        print(training_dt['sales'][a], training_dt['max_calcbk'][a])\n",
    "        training_set_more.append((training_dt['sales'][a], training_dt['max_calcbk'][a]))\n",
    "        labels.append('1')\n",
    "\n",
    "training_set_less = np.asarray(training_set_less)\n",
    "training_set_more = np.asarray(training_set_more)\n",
    "labels = np.asarray(labels)\n",
    "\n",
    "#print(training_set_less)\n",
    "#print(training_set_more)\n",
    "#print(labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(10, 10))\n",
    "\n",
    "#print(training_set_less[:,:2]) access 2nd column\n",
    "#print(np.size(training_set_more[:,:1]))\n",
    "#print(training_set_less[:,0])\n",
    "#print(training_set_less)\n",
    "\n",
    "#plt.scatter(training_set_less[:,0], training_set_less[:,1], color='b', label='0')\n",
    "#plt.scatter(training_set_more[:,0], training_set_more[:,1], color='r', label='1')\n",
    "#plt.legend();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['retail_outlet_number', 'base_product_number', 'calendar_date',\n",
       "       'calendar_hour', 'stock_count_shelf', 'stock_count_backroom',\n",
       "       'book_stock_quantity', 'change', 'change_cause', 'min_bookstock',\n",
       "       'max_bookstock', 'calculated_bookstock', 'change_cause_grouped'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_bookstock.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pandas.core.groupby.generic.DataFrameGroupBy object at 0x1a28e25128>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd_bookstock['calendar_date'] #workds\n",
    "pd_bookstock.groupby('calendar_date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### semi-final preparation of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of days of sales data in main dataframe: 142\n",
      "9\n",
      "[ 4  3 13  2  9  2  4  4  1  6  2  7  5  4  9  1 13 12  1  5  2 13  3  4\n",
      "  3  3  5  5  1  9  6 11  5  2  5  4  3  6  3  7  3  2  1  2  6  6  6  3\n",
      "  6  5  4  5  2  6  3  4  4  9  4  2  2  4  6  5  2  6  5  3  2  5  3]\n"
     ]
    }
   ],
   "source": [
    "print(\"Total number of days of sales data in main dataframe:\",size_days)\n",
    "#sum_change[:75] #accessing the first 75 entries of table sum_change\n",
    "#print(sum_change['change']) #accessing the column\n",
    "print(training_dt['sales'][0]) #accessing a column value by its index\n",
    "\n",
    "#Create training set\n",
    "training_set_less = []\n",
    "training_set_more = []\n",
    "labels = []\n",
    "\n",
    "for a in range(np.size(training_dt['calendar_date'])):\n",
    "    if(training_dt['sales'][a]<5):\n",
    "#        print(sum_change['sales'][a], sum_change['max_calcbk'][a])\n",
    "        training_set_less.append((training_dt['sales'][a], '0'))\n",
    "        labels.append('0')\n",
    "\n",
    "for a in range(np.size(training_dt['calendar_date'])):\n",
    "    if(training_dt['sales'][a]>=5):\n",
    "#        print(sum_change['sales'][a], sum_change['max_calcbk'][a])\n",
    "        training_set_more.append((training_dt['sales'][a], '1'))\n",
    "        labels.append('1')\n",
    "\n",
    "training_set_less = np.asarray(training_set_less)\n",
    "training_set_more = np.asarray(training_set_more)\n",
    "labels = np.asarray(labels)\n",
    "#######################################################################\n",
    "\n",
    "#######################################################################\n",
    "#TESTING SET\n",
    "\n",
    "testingdt_sales_values= testing_dt['sales'].values\n",
    "print(testingdt_sales_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### visualizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(training_set_less[:,0], training_set_less[:,1], color='b', label='0')\n",
    "plt.scatter(training_set_more[:,0], training_set_more[:,1], color='r', label='1')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### final preparation of the data and labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2' '2' '4' '2' '4' '3' '4' '4' '2' '2' '1' '1' '3' '2' '3' '2' '3' '2'\n",
      " '2' '4' '3' '4' '3' '3' '4' '4' '2' '3' '1' '3' '3' '2' '1' '4' '4' '4'\n",
      " '1' '4' '9' '5' '5' '9' '10' '8' '5' '6' '6' '11' '7' '9' '10' '10' '5'\n",
      " '9' '6' '11' '12' '5' '5' '8' '5' '9' '10' '9' '8' '9' '10' '5' '8' '6'\n",
      " '6']\n",
      "['0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0' '0'\n",
      " '0' '0' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1'\n",
      " '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1' '1']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "joint_training = np.concatenate((training_set_less, training_set_more), axis=0) #join both arrays\n",
    "training_data=joint_training[:,0] #this is the training data\n",
    "training_labels=joint_training[:,1] #the corresponding labels\n",
    "print(training_data)\n",
    "print(training_labels)\n",
    "#print(np.size(data))\n",
    "#print(data[:,None])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### fitting the model to labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 5.44 ms, sys: 5.87 ms, total: 11.3 ms\n",
      "Wall time: 22.5 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Ganesh/anaconda2/envs/py36/lib/python3.6/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1e+20, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LogisticRegression(C=1e20) #C is the inverse of regularization\n",
    "#print(model)\n",
    "\n",
    "#Fit the training data to the labels\n",
    "%time model.fit(training_data[:,None], training_labels)  #supply data in the form of a 2D array \"data[:,None]\" to fit(). 1D array gives an error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 4  3 13  2  9  2  4  4  1  6  2  7  5  4  9  1 13 12  1  5  2 13  3  4\n",
      "  3  3  5  5  1  9  6 11  5  2  5  4  3  6  3  7  3  2  1  2  6  6  6  3\n",
      "  6  5  4  5  2  6  3  4  4  9  4  2  2  4  6  5  2  6  5  3  2  5  3]\n",
      "['0' '0' '1' '0' '1' '0' '0' '0' '0' '1' '0' '1' '1' '0' '1' '0' '1' '1'\n",
      " '0' '1' '0' '1' '0' '0' '0' '0' '1' '1' '0' '1' '1' '1' '1' '0' '1' '0'\n",
      " '0' '1' '0' '1' '0' '0' '0' '0' '1' '1' '1' '0' '1' '1' '0' '1' '0' '1'\n",
      " '0' '0' '0' '1' '0' '0' '0' '0' '1' '1' '0' '1' '1' '0' '0' '1' '0']\n"
     ]
    }
   ],
   "source": [
    "print(testingdt_sales_values)\n",
    "\n",
    "pred_labels = model.predict(testingdt_sales_values[:,None])\n",
    "print(pred_labels)\n",
    "#model.predict(testingdt_sales_values[:,None])[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-74.00621403]), array([[16.41247646]]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#the intercept in logistic regression is log of the odds value (remember the StatQuest video) on the Y-axis when\n",
    "#...the corresponding x-value is 0\n",
    "\n",
    "model.intercept_, model.coef_\n",
    "\n",
    "#The coefficients are represented in terms of log(odds) graph (from StatQuest video on coefficients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[20 19]\n",
      " [18 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.53      0.51      0.52        39\n",
      "           1       0.42      0.44      0.43        32\n",
      "\n",
      "   micro avg       0.48      0.48      0.48        71\n",
      "   macro avg       0.48      0.48      0.48        71\n",
      "weighted avg       0.48      0.48      0.48        71\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "pred_labels = model.predict(testingdt_sales_values[:,None])\n",
    "\n",
    "#print(confusion_matrix(training_labels, pred_labels))\n",
    "print(confusion_matrix(pred_labels, training_labels))\n",
    "\n",
    "print(classification_report(pred_labels, training_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Accuracy** = (TP + TN) /(TP + TN + FP + FN)\n",
    "\n",
    "**Precision** = (TP) / (TP + FP)\n",
    "\n",
    "**Recall** = (TP) / (TP + FN)\n",
    "\n",
    "**F1 Score** = (2 x Precision x Recall) / (Precision + Recall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### random_state\n",
    "\n",
    "If you use random_state=some_number, then you can guarantee that the output of Run 1 will be equal to the output of Run 2, i.e. your split will be always the same. It doesn't matter what the actual random_state number is 42, 0, 21, ... The important thing is that everytime you use 42, you will always get the same output the first time you make the split. This is useful if you want reproducible results, for example in the documentation, so that everybody can consistently see the same numbers when they run the examples. In practice I would say, you should set the random_state to some fixed number while you test stuff, but then remove it in production if you really need a random (and not a fixed) split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[25  0]\n",
      " [ 0 18]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        25\n",
      "           1       1.00      1.00      1.00        18\n",
      "\n",
      "   micro avg       1.00      1.00      1.00        43\n",
      "   macro avg       1.00      1.00      1.00        43\n",
      "weighted avg       1.00      1.00      1.00        43\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(sum_change['sales'].values, all_labels, test_size = 0.3, random_state = 42, shuffle = True)\n",
    "\n",
    "y_pred = model.predict (X_test[:,None])\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
